{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion detector\n",
    "\n",
    "**Participants:**\n",
    "- Roger Baiges Trilla\n",
    "- Adrià Cantarero Carreras\n",
    "\n",
    "**Subject:**\n",
    "- Processament del Llenguatge Humà\n",
    "\n",
    "**Course:** 2024\n",
    "\n",
    "**Degree:** Intel·ligència Artificial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary functions in order to avoid warnings on MacOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"overflow encountered in cast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the necessary dataset and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have the necessary datasets and tools from NLTK\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with all the reviews and their respective categories\n",
    "\n",
    "documents = [(list(mr.words(fileid)), category)\n",
    "             for category in mr.categories()\n",
    "             for fileid in mr.fileids(category)]\n",
    "data = [' '.join(words) for words, _ in documents]\n",
    "labels = [category for _, category in documents]\n",
    "\n",
    "# Divide the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPERVISED MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define the stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Extending the stopwords list with movie-related words\n",
    "movie_related_stopwords = [\n",
    "    'movie', 'film', 'cinema', 'actor', 'actress', 'director', 'scene', 'screen', 'dvd', 'blu-ray', \n",
    "    'hollywood', 'plot', 'story', 'character', 'characters', 'storyline', 'cinematography', 'production', \n",
    "    'performances', 'cast', 'role', 'roles', 'dialogue', 'narrative', 'franchise', 'sequel', 'prequel', \n",
    "    'soundtrack', 'score', 'genre', 'genres', 'trailer', 'trailers', 'review', 'reviews', 'viewer', \n",
    "    'viewers', 'audience', 'critic', 'critics', 'acting', 'screenplay', 'visual', 'effects', 'cgi', \n",
    "    'theme', 'themes', 'setting', 'shot', 'shots', 'angle', 'angles', 'lighting', 'edit', 'edits', \n",
    "    'editing', 'version', 'versions', 'adaptation', 'series', 'sequels', 'prequels', 'installment', \n",
    "    'installments', 'chapter', 'chapters', 'release', 'releases', 'premiere', 'cameo', 'cameos', 'star', \n",
    "    'stars', 'starring', 'feature', 'featured', 'leads', 'supporting', 'extra', 'extras', 'portray', \n",
    "    'portrays', 'portrayed', 'portrayal', 'nominated', 'nomination', 'award', 'awards', 'win', 'wins', \n",
    "    'winner', 'losers', 'gross', 'box', 'office', 'ticket', 'budget', 'profit', 'profits', 'break', \n",
    "    'record', 'filmmaker', 'make', 'makes', 'made', 'creator', 'craft', 'shoot', 'shot', 'direct', \n",
    "    'produce', 'write', 'writes', 'edit', 'visuals', 'imagery', 'picture', 'photography', 'art', \n",
    "    'design', 'sound', 'music', 'melody', 'tune', 'background', 'style', 'classify', 'classified', \n",
    "    'classification', 'segment', 'section', 'part', 'feature', 'element', 'aspect', 'detail'\n",
    "]\n",
    "\n",
    "stop_words.update(movie_related_stopwords)\n",
    "\n",
    "def preprocessed_data(document):\n",
    "    \"\"\"\n",
    "    Preprocesses a given document by removing numbers, punctuation, converting to lowercase,\n",
    "    tokenizing, removing stopwords, and lemmatizing the tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - document (str): The document to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The preprocessed document as a single string.\n",
    "    \"\"\"\n",
    "    # Remove numbers\n",
    "    document = re.sub(r'\\d+', '', document)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    \n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(document)\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Remove stopwords and lemmatize the tokens\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Combine the filtered tokens back into a single string\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply the improved_preprocess function to the training and testing data\n",
    "data_train_processed = [preprocessed_data(doc) for doc in X_train]\n",
    "data_test_processed = [preprocessed_data(doc) for doc in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to convert data to a matrix of token counts\n",
    "vectorizer = CountVectorizer(min_df=5)\n",
    "data_train_processed = vectorizer.fit_transform(data_train_processed)\n",
    "data_test_processed = vectorizer.transform(data_test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE THE CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Initialize the classifiers    \n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "knn_clf = KNeighborsClassifier()\n",
    "svm_clf = SVC(random_state=42)\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "lr_clf = LogisticRegression(random_state=42)\n",
    "nb_clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEARCH THE BEST HYPERPARAMETERS WITH CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grids\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'lbfgs', 'newton-cg'],\n",
    "    'max_iter': [200, 300, 400]\n",
    "}\n",
    "\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0, 50.0]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Map model names to their corresponding grid\n",
    "param_grid_dict = {\n",
    "    \"Random Forest\": rf_param_grid,\n",
    "    \"K-Nearest Neighbors\": knn_param_grid,\n",
    "    \"SVM\": svm_param_grid,\n",
    "    \"XGBoost\": xgb_param_grid,\n",
    "    \"Logistic Regression\": lr_param_grid,\n",
    "    \"Naive Bayes\": nb_param_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": (rf_clf, False),\n",
    "    \"K-Nearest Neighbors\": (knn_clf, False),\n",
    "    \"SVM\": (svm_clf, False),\n",
    "    \"XGBoost\": (xgb_clf, True),\n",
    "    \"Logistic Regression\": (lr_clf, False),\n",
    "    \"Naive Bayes\": (nb_clf, False)\n",
    "}\n",
    "\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using GridSearchCV and cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to tune.\n",
    "    - param_grid (dict): The grid of hyperparameters to search.\n",
    "    - X_train: Training features.\n",
    "    - y_train: Training labels.\n",
    "\n",
    "    Returns:\n",
    "    - The best estimator with optimized hyperparameters.\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# Iterate over each model and tune hyperparameters\n",
    "for model_name, (model, encode_target) in classifiers.items():\n",
    "    # If the model requieres label encoding, encode the target labels\n",
    "    if encode_target:\n",
    "        encoder = LabelEncoder()\n",
    "        y_train_encoded = encoder.fit_transform(y_train)\n",
    "        print(f\"Hyperparameter tuning for {model_name}...\")\n",
    "        param_grid = param_grid_dict[model_name]\n",
    "        best_model = tune_hyperparameters(model, param_grid, data_train_processed, y_train_encoded)\n",
    "    else:\n",
    "        print(f\"Hyperparameter tuning for {model_name}...\")\n",
    "        param_grid = param_grid_dict[model_name]\n",
    "        best_model = tune_hyperparameters(model, param_grid, data_train_processed, y_train)\n",
    "    \n",
    "    # Update the classifiers dictionary with the best model\n",
    "    classifiers[model_name] = (best_model, encode_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATE MODEL FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, encode_target=False):\n",
    "    \"\"\"\n",
    "    Evaluates a given model on test data and returns evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to evaluate.\n",
    "    - X_train, X_test: Training and test features.\n",
    "    - y_train, y_test: Training and test labels.\n",
    "    - encode_target (bool): Flag to indicate if the model is XGBoost, requiring label encoding.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of (conf_matrix, accuracy, precision, f1)\n",
    "    \"\"\"\n",
    "    if encode_target:\n",
    "        encoder = LabelEncoder()\n",
    "        y_train_encoded = encoder.fit_transform(y_train)\n",
    "        model.fit(X_train, y_train_encoded)\n",
    "        predictions_encoded = model.predict(X_test)\n",
    "        predictions = encoder.inverse_transform(predictions_encoded)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, predictions, average='macro', zero_division=0)\n",
    "\n",
    "    # Identifying mistakes\n",
    "    mistakes = [(index, actual, pred) for index, (actual, pred) in enumerate(zip(y_test, predictions)) if actual != pred]\n",
    "\n",
    "    return conf_matrix, accuracy, precision, f1, mistakes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN THE FINAL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each model and evaluate it on the test data\n",
    "\n",
    "mistakes_dict = {}\n",
    "\n",
    "for model_name, (model, encode_target) in classifiers.items():\n",
    "    \"\"\"\n",
    "    For each model, evaluate it on the test data and display the confusion matrix.\n",
    "    \"\"\"\n",
    "    conf_matrix, accuracy, precision, f1, mistakes = evaluate_model(model, data_train_processed, data_test_processed, y_train, y_test, encode_target)\n",
    "    \n",
    "    # Store the mistakes in a dictionary\n",
    "    mistakes_dict[model_name] = mistakes\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 12}, cmap=\"Blues\", fmt=\"g\", xticklabels=True, yticklabels=True)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.savefig(f'./images/confusion_matrix_{model_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Evaluation Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS OF THE BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "\n",
    "best_model_name = max(classifiers, key=lambda x: classifiers[x][0].score(data_test_processed, y_test))\n",
    "best_model = classifiers[best_model_name][0]\n",
    "\n",
    "print(f\"The best model is: {best_model_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVIEWS THAT WERE MISCLASSIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mistakes made by the best model\n",
    "\n",
    "print(f\"Mistakes made by the best model ({best_model_name}):\\n\")\n",
    "for mistake in mistakes_dict[best_model_name]:\n",
    "    index, actual, pred = mistake\n",
    "    print(f\"Actual: {actual}, Predicted: {pred}\")\n",
    "    print(f\"Review: {X_test[index]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION TO PREDICT THE SENTIMENT OF A GIVEN REVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, model, vectorizer):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a given review using the best model.\n",
    "\n",
    "    Parameters:\n",
    "    - review (str): The review to predict the sentiment for.\n",
    "    - model: The best model for sentiment analysis.\n",
    "    - vectorizer: The CountVectorizer object used to transform the data.\n",
    "\n",
    "    Returns:\n",
    "    - str: The predicted sentiment of the review.\n",
    "    \"\"\"\n",
    "    # Preprocess the review\n",
    "    review_processed = preprocessed_data(review)\n",
    "    \n",
    "    # Vectorize the preprocessed review\n",
    "    review_vectorized = vectorizer.transform([review_processed])\n",
    "    \n",
    "    # Predict the sentiment\n",
    "    prediction = model.predict(review_vectorized)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Test the function with a sample review\n",
    "review = \"This movie was terrible! The acting was bad and the special effects were horrible.\"\n",
    "\n",
    "# Positive review (very long)\n",
    "\n",
    "sentiment = predict_sentiment(review, best_model, vectorizer)\n",
    "print(f\"Predicted sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION TO PLOT THE ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(model, X, y):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve of the given model without using numpy for label conversion.\n",
    "    Assumes 'pos' is the positive class and 'neg' is the negative class.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model that should have a `predict_proba` method.\n",
    "    - X: Feature matrix for prediction.\n",
    "    - y: Actual labels, expected to be 'pos' or 'neg'.\n",
    "    \"\"\"\n",
    "    # Preprocess labels: Convert 'pos' -> 1, 'neg' -> 0\n",
    "    y_numeric = [1 if label == 'pos' else 0 for label in y]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_scores = model.predict_proba(X)[:, 1]  # Assuming the positive class is at index 1\n",
    "    \n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_numeric, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plotting the ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:0.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('./images/best_model_roc_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the ROC curve for the best model\n",
    "plot_roc_curve(best_model, data_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS WITH SENTIWORDNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apliquem UKB per obtenir els synsets  https://textserver.cs.upc.edu/textserver/news\n",
    "from textserver import TextServer\n",
    "\n",
    "ts = TextServer('BaigesAndCanta', 'UKBserver-123', 'senses') \n",
    "ts.senses(\"L'Arnau té un gos. Se l'estima molt.\", pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenim els valors SentiWordnet de cada synsets\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "# getting the wordnet synset\n",
    "synset = wn.synset('good.a.1')\n",
    "# getting the sentiwordnet synset\n",
    "sentiSynset = swn.senti_synset(synset.name())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
